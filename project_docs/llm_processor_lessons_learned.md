# LLM Processor Lessons Learned & Best Practices

## üéØ **Overview**
This document captures critical insights and solutions discovered during the development and troubleshooting of the Stuart Data Cleaning System's LLM processing pipeline.

## üîë **Key Lessons**

### **1. Configuration Management & Security**

#### **API Key Security**
- **CRITICAL**: Never commit API keys to version control
- **Problem**: OpenAI automatically disables exposed API keys
- **Solution**: Use environment variables for sensitive configuration
- **Implementation**: 
  ```bash
  # Windows PowerShell
  $env:OPENAI_API_KEY="your_key_here"
  
  # Linux/Mac
  export OPENAI_API_KEY="your_key_here"
  ```

#### **Configuration Structure**
- **Issue**: Nested config structures vs flattened dicts
- **Problem**: `config.__dict__` flattens nested structures incorrectly
- **Solution**: Pass config objects directly, not `config.__dict__`
- **Correct Usage**:
  ```python
  # ‚úÖ Correct
  data_processor = DataProcessor(config)
  export_processor = ExportProcessor(config)
  
  # ‚ùå Incorrect
  data_processor = DataProcessor(config.__dict__)
  export_processor = ExportProcessor(config.__dict__)
  ```

### **2. LLM Processor Architecture**

#### **Multi-Tier Fallback System**
- **Design**: OpenAI ‚Üí Ollama ‚Üí Rule-based
- **Benefits**: Combines speed (OpenAI) with reliability (Ollama)
- **Performance**: OpenAI ~90% confidence, Ollama ~70% confidence
- **Fallback Logic**: Automatic retry (3 attempts) before processor switch

#### **Processor Initialization**
- **Health Checks**: Essential for detecting LLM availability
- **Error Handling**: Graceful degradation when processors fail
- **Configuration Validation**: Verify API keys and endpoints before processing

### **3. Ollama Integration Insights**

#### **Model Selection**
- **Best Performing**: `qwen2.5:7b-instruct-q4_K_M` (balanced speed/quality)
- **Avoid**: `deepseek-r1:8b` (overthinking, verbose responses)
- **Consider**: `phi3:mini` (fast, good instruction following)

#### **API Parameters**
- **Critical Settings**:
  ```python
  "context": [],  # Reset context to prevent contamination
  "num_predict": 150,  # Optimal token limit for name parsing
  "temperature": 0.1  # Low temperature for consistent parsing
  ```

#### **Prompt Consistency**
- **Principle**: Use identical prompts across all LLM processors
- **Benefit**: Predictable, comparable results
- **Implementation**: Shared prompt building methods

### **4. Data Processing Pipeline**

#### **Column Name Transformation**
- **Issue**: `pandas.read_excel` transforms column names
- **Problem**: `Contact Name` ‚Üí `contact_name` (lowercase, no spaces)
- **Solution**: Use transformed names consistently throughout pipeline
- **Pattern**: Check for both original and transformed names

#### **Header Detection**
- **Logic**: Auto-detect headers (row 0 for test, row 2 for production)
- **Implementation**: Intelligent fallback for different data formats
- **Debugging**: Extensive logging for troubleshooting

### **5. Error Handling & Debugging**

#### **LLM Response Parsing**
- **Robust Parsing**: Handle malformed JSON, comments, incomplete responses
- **Fallback Logic**: Return `None` for unparseable responses
- **Input Validation**: Check for empty/NaN values before LLM calls

#### **Progress Reporting**
- **Real-time Updates**: Show processing status, methods, confidence scores
- **Error Tracking**: Log failures with context for debugging
- **Performance Metrics**: Track success rates and processing times

## üõ†Ô∏è **Technical Solutions**

### **Excel Output Formatting**
- **Column Order**: Strict adherence to specified column sequence
- **Internal Columns**: Exclude processing metadata from output
- **Formatting**: Phone numbers, state cleaning, proper data types
- **Sheet Structure**: Contact Export + Removed with removal reasons

### **Data Lineage & Traceability**
- **"Removed as Remaining" Approach**: Start with all data in Removed sheet
- **Removal Reasons**: Human-readable explanations for data exclusion
- **Audit Trail**: Complete processing history and decision tracking

## üìä **Performance Optimizations**

### **Memory Efficiency**
- **Lazy Evaluation**: Process removed rows only when needed
- **Vectorized Operations**: Use pandas vectorized methods
- **Batch Processing**: Configurable batch sizes for large datasets

### **Processing Speed**
- **Parallel Processing**: Multi-worker support for large datasets
- **Retry Logic**: Configurable retry attempts with delays
- **Fallback Chains**: Fast failure detection and processor switching

## üîç **Troubleshooting Guide**

### **Common Issues & Solutions**

#### **"Ollama response missing expected keys: []"**
- **Cause**: Empty or malformed JSON response
- **Solution**: Input validation, robust JSON parsing
- **Prevention**: Health checks, model selection

#### **"KeyError: None of [Index(['Contact Name', 'Customer Name', 'Email'])] are in [columns]"**
- **Cause**: Column name transformation during data loading
- **Solution**: Use transformed column names consistently
- **Prevention**: Header detection and column mapping

#### **Excel Repair Errors**
- **Cause**: Invalid custom number formats
- **Solution**: Use standard Excel formatting
- **Prevention**: Validate Excel syntax before writing

### **Debugging Commands**
```python
# Check column names
print("Columns:", df.columns.tolist())

# Verify data structure
print("Shape:", df.shape)

# Inspect specific rows
print("Sample data:", df.head(2).to_dict('records'))
```

## üöÄ **Best Practices Summary**

1. **Security First**: Environment variables for API keys
2. **Configuration Objects**: Pass config directly, not flattened dicts
3. **Prompt Consistency**: Identical prompts across all LLM processors
4. **Robust Parsing**: Handle malformed responses gracefully
5. **Input Validation**: Check data before LLM processing
6. **Column Mapping**: Use transformed names consistently
7. **Error Handling**: Graceful degradation and fallback chains
8. **Progress Tracking**: Real-time updates and performance metrics
9. **Testing**: Comprehensive testing with multiple datasets
10. **Documentation**: Record lessons learned for future reference

## üìù **Future Improvements**

- **Model Fine-tuning**: Custom models for specific data cleaning tasks
- **Prompt Engineering**: Optimize prompts for better accuracy
- **Performance Monitoring**: Real-time cost and performance tracking
- **Automated Testing**: CI/CD pipeline for regression testing
- **User Interface**: Web-based configuration and monitoring dashboard
